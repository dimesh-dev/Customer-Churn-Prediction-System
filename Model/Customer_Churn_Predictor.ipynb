{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Telco Customer Churn Prediction\n",
                "\n",
                "## Task 1: Exploratory Data Analysis (EDA)\n",
                "This notebook covers the step-by-step process of analyzing the Telco Customer Churn dataset, preprocessing the data, building classification models (Decision Tree and Neural Network), and evaluating them.\n",
                "\n",
                "### 1.1 Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout\n",
                "\n",
                "# Set visualization style\n",
                "sns.set_style('whitegrid')\n",
                "%matplotlib inline\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
                "print(f\"Dataset Shape: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.3 Data Cleaning & Inspection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 'TotalCharges' is object but should be numeric. Coerce errors to NaN.\n",
                "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
                "\n",
                "# Check for missing values\n",
                "print(\"Missing values per column:\")\n",
                "print(df.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop rows with missing TotalCharges (usually very few)\n",
                "df.dropna(inplace=True)\n",
                "\n",
                "# Remove 'customerID' as it's not a feature\n",
                "if 'customerID' in df.columns:\n",
                "    df.drop(columns=['customerID'], inplace=True)\n",
                "\n",
                "print(\"New Shape after cleaning:\", df.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.4 Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target Variable Distribution\n",
                "plt.figure(figsize=(6,4))\n",
                "sns.countplot(x='Churn', data=df, palette='viridis')\n",
                "plt.title('Distribution of Churn')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Numerical Features Distributions\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "sns.histplot(df['tenure'], kde=True, ax=axes[0], color='skyblue')\n",
                "axes[0].set_title('Tenure Distribution')\n",
                "\n",
                "sns.histplot(df['MonthlyCharges'], kde=True, ax=axes[1], color='salmon')\n",
                "axes[1].set_title('Monthly Charges Distribution')\n",
                "\n",
                "sns.histplot(df['TotalCharges'], kde=True, ax=axes[2], color='green')\n",
                "axes[2].set_title('Total Charges Distribution')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note: Ensure column names match (case-sensitivity). The dataset usually has 'tenure' (lowercase) or 'Tenure'. Adjusting code to be safe."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standardizing column names to lowercase for ease\n",
                "df.columns = [c.lower() for c in df.columns]\n",
                "print(df.columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Churn vs Contract Type\n",
                "plt.figure(figsize=(8,5))\n",
                "sns.countplot(x='contract', hue='churn', data=df, palette='pastel')\n",
                "plt.title('Churn Rate by Contract Type')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation Matrix\n",
                "# Convert Churn to binary for correlation visualization\n",
                "df_corr = df.copy()\n",
                "df_corr['churn'] = df_corr['churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
                "numeric_df = df_corr.select_dtypes(include=['number'])\n",
                "\n",
                "plt.figure(figsize=(10,8))\n",
                "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
                "plt.title('Correlation Matrix')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 2: Model Implementation\n",
                "\n",
                "### 2.1 Data Preprocessing\n",
                "- Encoding Categorical Variables\n",
                "- Feature Scaling\n",
                "- Train-Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Drop Target from features\n",
                "X = df.drop('churn', axis=1)\n",
                "y = df['churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
                "\n",
                "# 2. Encoding Categorical Variables\n",
                "# Get dummy variables for categorical features, drop_first to avoid multicollinearity\n",
                "X = pd.get_dummies(X, drop_first=True)\n",
                "\n",
                "# 3. Scaling Numerical Features\n",
                "# Identify numerical cols: tenure, monthlycharges, totalcharges\n",
                "num_cols = ['tenure', 'monthlycharges', 'totalcharges']\n",
                "scaler = StandardScaler()\n",
                "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
                "\n",
                "# 4. Train-Test Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "print(f\"Training Shape: {X_train.shape}\")\n",
                "print(f\"Testing Shape: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Decision Tree Classifier\n",
                "- Implementation\n",
                "- Hyperparameter Tuning (GridSearchCV)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Init Model\n",
                "dt = DecisionTreeClassifier(random_state=42)\n",
                "\n",
                "# Hyperparameter Grid\n",
                "param_grid = {\n",
                "    'max_depth': [3, 5, 7, 10, None],\n",
                "    'min_samples_split': [2, 5, 10],\n",
                "    'criterion': ['gini', 'entropy']\n",
                "}\n",
                "\n",
                "# Grid Search\n",
                "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "best_dt = grid_search.best_estimator_\n",
                "print(\"Best Parameters for Decision Tree:\", grid_search.best_params_)\n",
                "\n",
                "# Predictions\n",
                "y_pred_dt = best_dt.predict(X_test)\n",
                "\n",
                "# Evaluation\n",
                "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
                "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_dt))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Neural Network Classifier\n",
                "- Implementation using TensorFlow/Keras\n",
                "- Model Architecture: Input -> Dense(ReLU) -> Dropout -> Dense(ReLU) -> Output(Sigmoid)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Sequential()\n",
                "\n",
                "# Input Layer & 1st Hidden Layer\n",
                "model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
                "model.add(Dropout(0.1))\n",
                "\n",
                "# 2nd Hidden Layer\n",
                "model.add(Dense(16, activation='relu'))\n",
                "model.add(Dropout(0.1))\n",
                "\n",
                "# Output Layer\n",
                "model.add(Dense(1, activation='sigmoid'))\n",
                "\n",
                "# Compile\n",
                "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
                "\n",
                "# Train\n",
                "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot Training History\n",
                "plt.figure(figsize=(10,4))\n",
                "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
                "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
                "plt.title('Neural Network Training History')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluation\n",
                "y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
                "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred_nn))\n",
                "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_nn))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.4 Model Comparison (ROC-AUC)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Decision Tree Probabilities\n",
                "y_prob_dt = best_dt.predict_proba(X_test)[:, 1]\n",
                "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt)\n",
                "auc_dt = auc(fpr_dt, tpr_dt)\n",
                "\n",
                "# Neural Network Probabilities\n",
                "y_prob_nn = model.predict(X_test).ravel()\n",
                "fpr_nn, tpr_nn, _ = roc_curve(y_test, y_prob_nn)\n",
                "auc_nn = auc(fpr_nn, tpr_nn)\n",
                "\n",
                "plt.figure(figsize=(8,6))\n",
                "plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {auc_dt:.2f})')\n",
                "plt.plot(fpr_nn, tpr_nn, label=f'Neural Network (AUC = {auc_nn:.2f})')\n",
                "plt.plot([0, 1], [0, 1], 'k--')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curve Comparison')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 3: Ethics & Post-Deployment Strategy\n",
                "\n",
                "### 3.1 AI Ethics Strategies\n",
                "**1. Fairness & Bias Mitigation:**\n",
                "- **Strategy:** Analyze the model's performance across different demographic groups (e.g., gender, senior citizen status) to ensure it doesn't systematically disadvantage a specific group.\n",
                "- **Implementation:** Use metrics like Disparate Impact or Equal Opportunity difference.\n",
                "\n",
                "**2. Transparency & Explainability:**\n",
                "- **Strategy:** Ensure stakeholders understand *why* a customer is predicted to churn.\n",
                "- **Implementation:** The Decision Tree naturally offers interpretability via feature importance. For the Neural Network, techniques like SHAP or LIME can be used.\n",
                "\n",
                "**3. Privacy & Data Protection:**\n",
                "- **Strategy:** Anonymize PII (Personally Identifiable Information) before training. The dataset uses 'customerID' which is a pseudonym, but further checks ensure no sensitive data leaks into the model.\n",
                "\n",
                "### 3.2 Post-Deployment Strategy\n",
                "**1. Model Monitoring:**\n",
                "- **Metric Tracking:** Continuously monitor Accuracy, Recall, and Precision in production. A drop in these metrics indicates problems.\n",
                "- **Data Drift Detection:** Compare the statistical distribution of incoming live data with the training data. If 'MonthlyCharges' distribution shifts significantly, the model may need retraining.\n",
                "\n",
                "**2. Retraining Pipeline:**\n",
                "- Establish a schedule (e.g., monthly) or trigger-based system (e.g., performance drops < 80%) to retrain the model with the latest data.\n",
                "\n",
                "**3. Feedback Loop:**\n",
                "- Incorporate actual churn results back into the dataset to refine future predictions."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}